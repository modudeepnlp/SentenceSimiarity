{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "snli.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1VLXxABMmSg",
        "colab_type": "text"
      },
      "source": [
        "# OS 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRw300PbMVDe",
        "colab_type": "code",
        "outputId": "ec3af660-0d31-4527-8c4f-a5d7f08b2449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!cat /etc/issue.net"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ubuntu 18.04.2 LTS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7Ri7zumMtUY",
        "colab_type": "text"
      },
      "source": [
        "# 하드웨어 사양"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lrWIstoMxmg",
        "colab_type": "code",
        "outputId": "3de84e30-79ca-4432-961a-8a36b1ddf7f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LII9PLC3M2FK",
        "colab_type": "text"
      },
      "source": [
        "# Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qfx1fyUM64Q",
        "colab_type": "code",
        "outputId": "d4b59f72-f0f5-4d73-f0b1-6def0861d4fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "!cat /proc/meminfo"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MemTotal:       13335276 kB\n",
            "MemFree:        11166932 kB\n",
            "MemAvailable:   12649152 kB\n",
            "Buffers:           81636 kB\n",
            "Cached:          1556140 kB\n",
            "SwapCached:            0 kB\n",
            "Active:           551212 kB\n",
            "Inactive:        1364648 kB\n",
            "Active(anon):     248336 kB\n",
            "Inactive(anon):      352 kB\n",
            "Active(file):     302876 kB\n",
            "Inactive(file):  1364296 kB\n",
            "Unevictable:           0 kB\n",
            "Mlocked:               0 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:               904 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:        278148 kB\n",
            "Mapped:           169380 kB\n",
            "Shmem:               904 kB\n",
            "Slab:             144040 kB\n",
            "SReclaimable:     112364 kB\n",
            "SUnreclaim:        31676 kB\n",
            "KernelStack:        3680 kB\n",
            "PageTables:         4632 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6667636 kB\n",
            "Committed_AS:    1733252 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:           0 kB\n",
            "VmallocChunk:          0 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "DirectMap4k:       90100 kB\n",
            "DirectMap2M:     6201344 kB\n",
            "DirectMap1G:     9437184 kB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wek8XKkZM-NB",
        "colab_type": "text"
      },
      "source": [
        "# Disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK5RySL_NAU5",
        "colab_type": "code",
        "outputId": "b48c960b-7e2e-4bf6-9213-daf926ef72bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!df -h"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         359G   25G  316G   8% /\n",
            "tmpfs           6.4G     0  6.4G   0% /dev\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/fs/cgroup\n",
            "tmpfs           6.4G   12K  6.4G   1% /var/colab\n",
            "/dev/sda1       365G   29G  336G   8% /opt/bin\n",
            "shm             6.0G     0  6.0G   0% /dev/shm\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--FvI1fSNEoM",
        "colab_type": "text"
      },
      "source": [
        "# GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79wMqVvlNG3x",
        "colab_type": "code",
        "outputId": "a3af5b3c-e5f4-48d2-e5d8-65335f35dbe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue May 21 06:11:29 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oAn6EPON4P1",
        "colab_type": "text"
      },
      "source": [
        "# 구글 드라이브와 Colab연동"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkNUVhymN8D6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0PtvBskOPTs",
        "colab_type": "code",
        "outputId": "8844753d-23fa-4499-8e4f-ae344d8ae920",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!cd 'gdrive/My Drive/data/snli_1.0'; ls -al;"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 876986\n",
            "-rw------- 1 root root      5828 May 20 08:38 README.txt\n",
            "-rw------- 1 root root   9745714 May 20 08:39 snli_1.0_dev.jsonl\n",
            "-rw------- 1 root root   7565773 May 20 08:39 snli_1.0_dev.txt\n",
            "-rw------- 1 root root   9730457 May 20 08:39 snli_1.0_test.jsonl\n",
            "-rw------- 1 root root   7550390 May 20 08:39 snli_1.0_test.txt\n",
            "-rw------- 1 root root 487457790 May 20 08:40 snli_1.0_train.jsonl\n",
            "-rw------- 1 root root 375697923 May 20 08:39 snli_1.0_train.txt\n",
            "-rw------- 1 root root    277673 May 20 08:38 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKxKiLTntpvd",
        "colab_type": "text"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-TDDGfjWZlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DMfgBNottPV",
        "colab_type": "text"
      },
      "source": [
        "# Define training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N97SCqKUNX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regxs = list(\"[@_!#$%^&*()<>?/\\|}{~:]'\\\".,-;`+=\")\n",
        "\n",
        "def strip_string(string):\n",
        "    for regx in regxs:\n",
        "        string = string.replace(regx, \" \" + regx + \" \")\n",
        "    return string.lower().strip()\n",
        "\n",
        "  \n",
        "# label index 정의\n",
        "label_dict = { \"neutral\": 0, \"entailment\": 1, \"contradiction\": 2 }\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "pickle로 vocab 로드\n",
        "\"\"\"\n",
        "def load_vocab(file=\"Data/vocab.txt\"):\n",
        "    vocab = {}\n",
        "    index = 0\n",
        "    with open(file, \"r\") as f:\n",
        "        for token in f:\n",
        "            vocab[token.strip()] = index\n",
        "            index += 1\n",
        "    return vocab\n",
        "\n",
        "  \n",
        "\"\"\"\n",
        "파일로 부터 vocab을 이용해 데이터 로드\n",
        "\"\"\"\n",
        "def load_data(file, vocab):\n",
        "    gold_label = []\n",
        "    sentence1 = []\n",
        "    sentence2 = []\n",
        "\n",
        "    dataset = pd.read_csv(file, sep=\"\\t\")\n",
        "    for i, row in dataset.iterrows():\n",
        "        if row['gold_label'] == \"-\" or pd.isnull(row['sentence1']) or pd.isnull(row['sentence2']):\n",
        "            continue\n",
        "\n",
        "        gold_label.append(label_dict[row['gold_label']])\n",
        "\n",
        "        line1 = []\n",
        "        for token in strip_string(row['sentence1']).split():\n",
        "            line1.append(vocab[token])\n",
        "        while len(line1) < 82:\n",
        "            line1.append(1) # <pad>\n",
        "        sentence1.append(line1)\n",
        "\n",
        "        line2 = []\n",
        "        for token in strip_string(row['sentence2']).split():\n",
        "            line2.append(vocab[token])\n",
        "        while len(line2) < 63:\n",
        "            line2.append(1) # <pad>\n",
        "        sentence2.append(line2)\n",
        "    \n",
        "    return gold_label, sentence1, sentence2\n",
        "\n",
        "\n",
        "def build_tensor(label, sentence1, sentence2, batch_size=256):\n",
        "    torch_labe = torch.tensor(label, dtype=torch.long)\n",
        "    torch_sentence1 = torch.tensor(sentence1, dtype=torch.long)\n",
        "    torch_sentence2 = torch.tensor(sentence2, dtype=torch.long)\n",
        "    dataset = torch.utils.data.TensorDataset(torch_labe, torch_sentence1, torch_sentence2)\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    return loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdAnikLDty18",
        "colab_type": "text"
      },
      "source": [
        "# defile SNLI Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNH7pDhVVUh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaseLine(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BaseLine, self).__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(config[\"n_embed\"], config[\"d_embed\"])\n",
        "        self.hbmp = HBMP(config)\n",
        "        self.layer1 = nn.Linear(config[\"d_embed\"], 32)\n",
        "        self.layer2 = nn.Linear(config[\"d_embed\"], 32)\n",
        "        self.output = nn.Linear(32, config[\"n_output\"])\n",
        "\n",
        "    def forward(self, sentence1, sentence2):\n",
        "#         sentence1_embed = self.embed(sentence1)\n",
        "        sentence1_embed = self.hbmp(sentence1)\n",
        "        sentence1_ctx = self.layer1(sentence1_embed)\n",
        "        sentence1_ctx = sentence1_ctx.mean(1)\n",
        "\n",
        "#         sentence2_embed = self.embed(sentence2)\n",
        "        sentence2_embed = self.hbmp(sentence2)\n",
        "        sentence2_ctx = self.layer2(sentence2_embed)\n",
        "        sentence2_ctx = sentence2_ctx.mean(1)\n",
        "\n",
        "        output = self.output(sentence1_ctx - sentence2_ctx)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ5eBvSlVXIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hierarchical BiLSTM Max Pooling\n",
        "class HBMP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(HBMP, self).__init__()\n",
        "        \n",
        "        self.config = config\n",
        "        \n",
        "        self.max_pool = nn.AdaptiveMaxPool1d(1)\n",
        "        self.rnn1 = nn.LSTM(input_size=config[\"n_embed\"],\n",
        "                            hidden_size=config[\"d_embed\"],\n",
        "                            num_layers=config[\"n_layer\"],\n",
        "                            dropout=config[\"dropout\"],\n",
        "                            bidirectional=True)\n",
        "        self.rnn2 = nn.LSTM(input_size=config[\"n_embed\"],\n",
        "                            hidden_size=config[\"d_embed\"],\n",
        "                            num_layers=config[\"n_layer\"],\n",
        "                            dropout=config[\"dropout\"],\n",
        "                            bidirectional=True)\n",
        "        self.rnn3 = nn.LSTM(input_size=config[\"n_embed\"],\n",
        "                            hidden_size=config[\"d_embed\"],\n",
        "                            num_layers=config[\"n_layer\"],\n",
        "                            dropout=config[\"dropout\"],\n",
        "                            bidirectional=True)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        batch_size = inputs.size()[1]\n",
        "        h_0 = c_0 = Variable(inputs.data.new(self.config[\"n_layer\"],\n",
        "                                             batch_size,\n",
        "                                             self.config[\"d_embed\"]).zero_())\n",
        "        out1, (ht1, ct1) = self.rnn1(inputs, (h_0, c_0))\n",
        "#         emb1 = self.max_pool(out1.permute(1,2,0)).permute(2,0,1)\n",
        "\n",
        "#         out2, (ht2, ct2) = self.rnn2(inputs, (ht1, ct1))\n",
        "#         emb2 = self.max_pool(out2.permute(1,2,0)).permute(2,0,1)\n",
        "\n",
        "#         out3, (ht3, ct3) = self.rnn3(inputs, (ht2, ct2))\n",
        "#         emb3 = self.max_pool(out3.permute(1,2,0)).permute(2,0,1)\n",
        "\n",
        "#         emb = torch.cat([emb1, emb2, emb3], 2)\n",
        "#         emb = emb.squeeze(0)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVzf5IXrt2IQ",
        "colab_type": "text"
      },
      "source": [
        "# Execute Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtcQANVlVoXY",
        "colab_type": "code",
        "outputId": "65cc1290-060a-46ea-a652-694a6c35aae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "source": [
        "vocab = load_vocab(\"gdrive/My Drive/data/snli_1.0/vocab.txt\")\n",
        "print(\"load_vocab .......................... %d\" % len(vocab))\n",
        "# train_labe, train_sentence1, train_sentence2 = load_data(\"gdrive/My Drive/data/snli_1.0/snli_1.0_train.txt\", vocab)\n",
        "train_labe, train_sentence1, train_sentence2 = load_data(\"gdrive/My Drive/data/snli_1.0/snli_1.0_test.txt\", vocab) ## fast test only\n",
        "print(\"load_data train ..................... %d\" % len(train_labe))\n",
        "dev_labe, dev_sentence1, dev_sentence2 = load_data(\"gdrive/My Drive/data/snli_1.0/snli_1.0_dev.txt\", vocab)\n",
        "print(\"load_data dev ....................... %d\" % len(dev_labe))\n",
        "test_labe, test_sentence1, test_sentence2 = load_data(\"gdrive/My Drive/data/snli_1.0/snli_1.0_test.txt\", vocab)\n",
        "print(\"load_data test ...................... %d\" % len(test_labe))\n",
        "\n",
        "config = {\n",
        "          \"n_embed\": len(vocab), \"d_embed\": 32, \"n_output\": 3, \"n_epoch\": 10, \"n_batch\": 256,\n",
        "          \"n_layer\": 1, \"dropout\": 0.1 ## HBMP\n",
        "         }\n",
        "snli = BaseLine(config=config)\n",
        "\n",
        "seed = 1029\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(snli.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "train_loader = build_tensor(train_labe, train_sentence1, train_sentence2, config[\"n_batch\"])\n",
        "dev_loader = build_tensor(dev_labe, dev_sentence1, dev_sentence2, config[\"n_batch\"])\n",
        "test_loader = build_tensor(test_labe, test_sentence1, test_sentence2, config[\"n_batch\"])\n",
        "\n",
        "epochs = []\n",
        "dev_score = []\n",
        "test_score = []\n",
        "\n",
        "for epoch in range(config[\"n_epoch\"]):\n",
        "    epochs.append(epoch + 1)\n",
        "    \n",
        "    #\n",
        "    # train\n",
        "    #\n",
        "    snli.train()\n",
        "    train_loss = 0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        batch_label, batch_sentence1, batch_sentence2 = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred_label = snli(batch_sentence1, batch_sentence2)\n",
        "        loss = loss_fn(pred_label, batch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "    train_loss = train_loss / len(train_loader)\n",
        "    \n",
        "    snli.eval()\n",
        "    #\n",
        "    # dev evaluate\n",
        "    #\n",
        "    dev_match = np.zeros((len(dev_labe)))\n",
        "    for i, data in enumerate(dev_loader, 0):\n",
        "        batch_label, batch_sentence1, batch_sentence2 = data\n",
        "        pred_label = snli(batch_sentence1, batch_sentence2)\n",
        "        _, indices = pred_label.max(1)\n",
        "        match = torch.eq(indices, batch_label).detach()\n",
        "        dev_match[i * config[\"n_batch\"]:(i+1) * config[\"n_batch\"]] = match\n",
        "    dev_accuracy = np.sum(dev_match) * 100 / len(dev_match)\n",
        "    dev_score.append(dev_accuracy)\n",
        "\n",
        "    #\n",
        "    # test evaluate\n",
        "    #\n",
        "    test_match = np.zeros((len(test_labe)))\n",
        "    for i, data in enumerate(test_loader, 0):\n",
        "        batch_label, batch_sentence1, batch_sentence2 = data\n",
        "        pred_label = snli(batch_sentence1, batch_sentence2)\n",
        "        _, indices = pred_label.max(1)\n",
        "        match = torch.eq(indices, batch_label).detach()\n",
        "        test_match[i * config[\"n_batch\"]:(i+1) * config[\"n_batch\"]] = match\n",
        "    test_accuracy = np.sum(test_match) * 100 / len(test_match)\n",
        "    test_score.append(test_accuracy)\n",
        "    \n",
        "    print(\"[%d], loss: %.3f, dev: %.3f, test: %.3f\" % (epoch + 1, train_loss, dev_accuracy, test_accuracy))\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data=np.array([dev_score, test_score]), columns=epochs, index=[\"dev\", \"test\"])\n",
        "display(df)\n",
        "\n",
        "plt.plot(epochs, dev_score, label='dev')\n",
        "plt.plot(epochs, test_score, label='test')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load_vocab .......................... 33827\n",
            "load_data train ..................... 9824\n",
            "load_data dev ....................... 9842\n",
            "load_data test ...................... 9824\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d19b680a106c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sentence2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-c25ea989432f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence1, sentence2)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#         sentence1_embed = self.embed(sentence1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0msentence1_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhbmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0msentence1_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0msentence1_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence1_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-7778863f8b3c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     27\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                                              self.config[\"d_embed\"]).zero_())\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mht1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m#         emb1 = self.max_pool(out1.permute(1,2,0)).permute(2,0,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;31m# type: (Tensor, Tuple[Tensor, Tensor], Optional[Tensor]) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    147\u001b[0m             raise RuntimeError(\n\u001b[1;32m    148\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[0;32m--> 149\u001b[0;31m                     expected_input_dim, input.dim()))\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 2"
          ]
        }
      ]
    }
  ]
}